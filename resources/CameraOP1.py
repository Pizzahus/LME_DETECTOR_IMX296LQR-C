import os
import cv2
import numpy
import pytesseract
import re

from picamera2 import Picamera2
from gpiozero import Button, LED
from PySide6.QtWidgets import QLabel
from PySide6.QtGui import QImage, QPixmap
from PySide6.QtCore import QThread, Slot, Signal
from time import time, sleep, perf_counter
from datetime import datetime
from dataclasses import dataclass
import collections
from resources.ConfigManager import ConfigManager, RectangleSettings

# ‡∏Å‡∏≥‡∏´‡∏ô‡∏î‡πÇ‡∏ü‡∏•‡πÄ‡∏î‡∏≠‡∏£‡πå‡∏ó‡∏µ‡πà‡∏à‡∏∞‡πÄ‡∏Å‡πá‡∏ö‡∏†‡∏≤‡∏û
capture_dir = "./captured_images"
os.makedirs(capture_dir, exist_ok=True)  # ‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÇ‡∏ü‡∏•‡πÄ‡∏î‡∏≠‡∏£‡πå‡∏´‡∏≤‡∏Å‡∏¢‡∏±‡∏á‡πÑ‡∏°‡πà‡∏°‡∏µ

pytesseract.pytesseract.tesseract_cmd = r"/usr/bin/tesseract"

rectColor = (255, 17, 0)


# ‡πÅ‡∏™‡∏î‡∏á‡∏†‡∏≤‡∏û‡∏à‡∏≤‡∏Å‡∏Å‡∏•‡πâ‡∏≠‡∏á
class CameraView(QThread):
    updateImage = Signal(QPixmap)
    updateDetectImage = Signal()

    def __init__(self, monitor: QLabel, camera: Picamera2, sensorPin: int, rectangle: RectangleSettings, flashLightPin: int, showFps: bool = False):
        super().__init__()
        self.monitor = monitor
        self.camera = camera
        self.sensor = Button(sensorPin, pull_up=True, bounce_time=0.05)  # ‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤ bounce_time = 0.1 ‡∏ß‡∏¥‡∏ô‡∏≤‡∏ó‡∏µ
        self.rectangle = rectangle
        # self.flashLight = LED(flashLightPin, active_high=True)  # ‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤ LED ‡πÉ‡∏´‡πâ‡∏ó‡∏≥‡∏á‡∏≤‡∏ô‡πÅ‡∏ö‡∏ö active low
        self.isLiveView = True
        self.is_triggered = False
        self.isShowRect = False
        self.showFps = showFps

        # config = self.camera.create_still_configuration(main={"size": (1024, 768)}, buffer_count=4, queue=False)
        config = self.camera.create_still_configuration(main={"size": (1024, 768)}, buffer_count=4, queue=False)
        self.camera.configure(config)

        self.camera.start()

        # ‡∏ß‡∏±‡∏î‡πÅ‡∏™‡∏á‡∏Å‡πà‡∏≠‡∏ô‡∏ñ‡πà‡∏≤‡∏¢‡∏†‡∏≤‡∏û‡∏à‡∏£‡∏¥‡∏á (Optional)
        metadata = self.camera.capture_metadata()
        print(f"Current Exposure: {metadata['ExposureTime']}, Gain: {metadata['AnalogueGain']}")

        # ‡∏õ‡∏¥‡∏î Auto Exposure ‡πÅ‡∏•‡∏∞‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤ ExposureTime, AnalogueGain ‡∏Ñ‡∏á‡∏ó‡∏µ‡πà
        controls = {
            # ‡∏û‡∏≤‡∏£‡∏≤‡∏°‡∏¥‡πÄ‡∏ï‡∏≠‡∏£‡πå‡∏û‡∏∑‡πâ‡∏ô‡∏ê‡∏≤‡∏ô
            "AeEnable": True,
            "ExposureTime": 10000,
            "AnalogueGain": 2,
            "AwbEnable": False,
            "Brightness": 0.2,
            # "Contrast": 1.8,
            # "Saturation": 1.2,
            # ‡∏û‡∏≤‡∏£‡∏≤‡∏°‡∏¥‡πÄ‡∏ï‡∏≠‡∏£‡πå‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏õ‡∏£‡∏∞‡∏™‡∏¥‡∏ó‡∏ò‡∏¥‡∏†‡∏≤‡∏û
            "Sharpness": 1.5,
            "FrameRate": 60,
            # "NoiseReductionMode": "HighQuality",
            # ‡∏û‡∏≤‡∏£‡∏≤‡∏°‡∏¥‡πÄ‡∏ï‡∏≠‡∏£‡πå‡πÅ‡∏™‡∏á
            # "ColourGains": (1.8, 1.5),
            # "AeMeteringMode": "CentreWeighted"
        }
        self.camera.set_controls(controls)

        # ‡∏≠‡∏±‡∏û‡πÄ‡∏î‡∏ó monitor
        self.updateImage.connect(self._update_pixmap)

    # ‡πÅ‡∏õ‡∏•‡∏á‡∏Ñ‡πà‡∏≤‡πÄ‡∏õ‡πá‡∏ô‡πÄ‡∏õ‡∏≠‡∏£‡πå‡πÄ‡∏ã‡πá‡∏ô‡∏ï‡πå
    def _mapValue(self, value, min=0, max=255):
        return f"{(((value-min) / (max-min)) * 100):.2f} %"

    @Slot(QPixmap)  # ‡∏≠‡∏±‡∏û‡πÄ‡∏î‡∏ó monitor
    def _update_pixmap(self, pixmap):
        self.monitor.setPixmap(pixmap)

    @Slot(QPixmap)  # ‡∏≠‡∏±‡∏û‡πÄ‡∏î‡∏ó detect monitor
    def _update_detect_pixmap(self, pixmap):
        self.monitor.setPixmap(pixmap)

    # ‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤‡πÉ‡∏´‡πâ‡πÅ‡∏™‡∏î‡∏á‡∏†‡∏≤‡∏û‡∏™‡∏î
    def liveView(self, value: bool):
        self.isLiveView = value

    # ‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤‡πÉ‡∏´‡πâ‡πÅ‡∏™‡∏î‡∏á‡∏†‡∏≤‡∏û‡∏ó‡∏µ‡πà‡∏à‡∏±‡∏ö‡πÑ‡∏î‡πâ
    def _detect_and_recognize_text(self, image):
        start_time = time()
        gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)
         # üîπ ‡πÄ‡∏ö‡∏•‡∏≠‡∏Å‡πà‡∏≠‡∏ô (‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡πÑ‡∏î‡πâ GaussianBlur ‡∏´‡∏£‡∏∑‡∏≠ MedianBlur)
        gray = cv2.GaussianBlur(gray, (5, 5), 0)   # GaussianBlur
        # gray = cv2.medianBlur(gray, 3)           # MedianBlur (‡πÄ‡∏´‡∏°‡∏≤‡∏∞‡∏Å‡∏±‡∏ö salt-pepper noise)
        gray = cv2.threshold(gray, 0, 255, cv2.THRESH_BINARY | cv2.THRESH_OTSU)[1]

        # ‡πÉ‡∏ä‡πâ Pytesseract ‡∏≠‡πà‡∏≤‡∏ô‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏° OCR ‡∏û‡∏£‡πâ‡∏≠‡∏°‡∏ï‡∏≥‡πÅ‡∏´‡∏ô‡πà‡∏á
        config = (
            r"--oem 1 --psm 6 "
            # r"-c tessedit_char_whitelist=0123456789/"
            r"-c tessedit_char_whitelist=0123456789/ABCDEFGHIJKLMNOPQRSTUVWXYZ"
            # r"tessedit_char_blacklist=ABCDEFGHIJKLMNOPQRSTUVWXYZ"
        )
        data = pytesseract.image_to_data(gray, lang="label", config=config, output_type=pytesseract.Output.DICT)

        # ‡∏ß‡∏≤‡∏î‡∏Å‡∏£‡∏≠‡∏ö‡∏£‡∏≠‡∏ö‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°
        # for i in range(len(data["text"])):
        #     text = data["text"][i]
        #     if text.strip():  # ‡∏Ç‡πâ‡∏≤‡∏°‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ß‡πà‡∏≤‡∏á
        #         x, y, w, h = data["left"][i], data["top"][i], data["width"][i], data["height"][i]
        #         cv2.rectangle(image, (x, y), (x + w, y + h), rectColor, 1)
        #         cv2.putText(image, text, (x, y - 5), cv2.FONT_HERSHEY_SIMPLEX, 0.4, rectColor, 1)

        results = []
        for i, text in enumerate(data["text"]):
            if int(data["conf"][i]) > 70:  # ‡∏Å‡∏£‡∏≠‡∏á‡∏Ñ‡∏ß‡∏≤‡∏°‡∏°‡∏±‡πà‡∏ô‡πÉ‡∏à > 70%
                results.append(text)

        # ‡∏ß‡∏≤‡∏î‡∏Å‡∏£‡∏≠‡∏ö‡∏£‡∏≠‡∏ö‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°
        for i in range(len(data["text"])):
            text = data["text"][i]
            if text.strip():  # ‡∏Ç‡πâ‡∏≤‡∏°‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ß‡πà‡∏≤‡∏á
                x, y, w, h = data["left"][i], data["top"][i], data["width"][i], data["height"][i]
                
                # ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì‡∏Ñ‡∏ß‡∏≤‡∏°‡∏Å‡∏ß‡πâ‡∏≤‡∏á‡∏Ç‡∏≠‡∏á‡πÅ‡∏ï‡πà‡∏•‡∏∞‡∏ï‡∏±‡∏ß‡∏≠‡∏±‡∏Å‡∏©‡∏£‡πÇ‡∏î‡∏¢‡∏õ‡∏£‡∏∞‡∏°‡∏≤‡∏ì
                char_width = w / len(text)
                
                # ‡∏ï‡∏µ‡∏Å‡∏£‡∏≠‡∏ö‡πÅ‡∏ï‡πà‡∏•‡∏∞‡∏ï‡∏±‡∏ß‡∏≠‡∏±‡∏Å‡∏©‡∏£
                for j, char in enumerate(text):
                    char_x = int(x + j * char_width)
                    char_w = int(char_width)
                    
                    # ‡∏ß‡∏≤‡∏î‡∏Å‡∏£‡∏≠‡∏ö‡∏£‡∏≠‡∏ö‡∏ï‡∏±‡∏ß‡∏≠‡∏±‡∏Å‡∏©‡∏£
                    cv2.rectangle(image, (char_x, y), (char_x + char_w, y + h), rectColor, 1)
                    
                    # ‡∏ß‡∏≤‡∏î‡∏ï‡∏±‡∏ß‡∏≠‡∏±‡∏Å‡∏©‡∏£ (optional)
                    cv2.putText(image, char, (char_x, y - 5), cv2.FONT_HERSHEY_SIMPLEX, 0.4, rectColor, 1)

        # ‡∏î‡∏∂‡∏á‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏°‡∏≤‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô‡∏ï‡πà‡∏≠
        text = " ".join(data["text"])

        # ‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤ Lot No.
        lot_no = re.search(r"\s*\|?\s*(\d{5})", text)
        lot_no = lot_no.group(1) if lot_no else None

        # ‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤ MFG / EXP
        dates = re.findall(r"\d{2}/\d{2}/\d{2}", text)
        mfg_date = dates[0] if len(dates) >= 1 else None
        exp_date = dates[1] if len(dates) >= 2 else None

        lme = (lot_no, mfg_date, exp_date)

        # ‡∏à‡∏±‡∏ö‡πÄ‡∏ß‡∏•‡∏≤‡∏ó‡∏µ‡πà‡∏™‡∏¥‡πâ‡∏ô‡∏™‡∏∏‡∏î
        end_time = time()
        processing_time = (end_time - start_time) * 1000

        # ‡∏û‡∏¥‡∏°‡∏û‡πå‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ó‡∏µ‡πà‡∏≠‡πà‡∏≤‡∏ô‡πÑ‡∏î‡πâ
        now = datetime.now()
        timestamp = now.strftime("%d/%m/%Y, %H:%M:%S")
        print("(Camera detected a message)=> ")
        print(f"Processing in: {processing_time:.2f}ms")
        print("Timestamp: ", timestamp)
        print("Results: ", text.split("\n"))
        print(f"Lot No.: {lot_no}")  # ‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå: 50756
        print(f"Mfg. date: {mfg_date}")  # ‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå: 19/05/25
        print(f"Exp. date: {exp_date}", "\n")  # ‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå: 19/05/27
        return (gray, timestamp, processing_time, lme)

    # ‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡∏à‡∏±‡∏ö‡∏†‡∏≤‡∏û
    def captured(self, isDetect=False):
        X1 = self.rectangle.X1
        Y1 = self.rectangle.Y1
        X2 = self.rectangle.X2
        Y2 = self.rectangle.Y2
        frame = self.camera.capture_array()

        if self.isShowRect:
            cv2.rectangle(frame, (X1, Y1), (X2, Y2), (255, 0, 0), 2)

        # Convert color space and ensure the array is contiguous
        frame = cv2.cvtColor(frame, cv2.COLOR_RGB2BGR)
        frame = numpy.ascontiguousarray(frame)  # This ensures C-contiguous memory layout
        
        height, width, channel = frame.shape
        bytes_per_line = channel * width
        q_img = QImage(
            frame.data,
            width,
            height,
            bytes_per_line,
            QImage.Format.Format_BGR888,
        )

        self.updateImage.emit(QPixmap.fromImage(q_img))

        # Crop image
        cropped_frame = frame[Y1:Y2, X1:X2]
        # inverted_image = cv2.bitwise_not(cropped_frame)

        # Detect and recognize text
        if isDetect:
            timestamp = datetime.now().strftime("%d%m%y_%H%M%S")
            image_filenameOriginal = f"captured_images/original/cap_{timestamp}.png"
            image_filenameProcess = f"captured_images/process/cap_{timestamp}.png"
            gray, timestamp, processing_time, text = self._detect_and_recognize_text(cropped_frame)
            # cv2.imwrite(image_filenameOriginal, gray)
            # cv2.imwrite(image_filenameProcess, gray)
            return timestamp, processing_time, text, cropped_frame
        else:
            return cropped_frame

    # ‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡∏à‡∏±‡∏ö‡∏†‡∏≤‡∏û‡πÅ‡∏•‡∏∞‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏•‡∏á‡πÑ‡∏ü‡∏•‡πå
    def saveCapturedImage(self, image, filename):
        # ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏†‡∏≤‡∏û‡∏ó‡∏µ‡πà‡∏à‡∏±‡∏ö‡πÑ‡∏î‡πâ‡∏•‡∏á‡πÉ‡∏ô‡πÇ‡∏ü‡∏•‡πÄ‡∏î‡∏≠‡∏£‡πå‡∏ó‡∏µ‡πà‡∏Å‡∏≥‡∏´‡∏ô‡∏î
        filepath = os.path.join(capture_dir, filename)
        cv2.imwrite(filepath, image)
        print(f"Image saved: {filepath}")

    # ‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡∏á‡∏≤‡∏ô‡∏Ç‡∏≠‡∏á‡∏Å‡∏•‡πâ‡∏≠‡∏á
    def run(self):
        self.thead_running = True
        frame_times = collections.deque(maxlen=60)  # ‡πÄ‡∏Å‡πá‡∏ö‡πÄ‡∏ß‡∏•‡∏≤ 60 ‡πÄ‡∏ü‡∏£‡∏°‡∏•‡πà‡∏≤‡∏™‡∏∏‡∏î
        try:
            while self.thead_running:
                if self.isLiveView:
                    self.captured()

                    if self.showFps:
                            # ‡πÇ‡∏Ñ‡πâ‡∏î‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡πÄ‡∏ü‡∏£‡∏°‡∏Ç‡∏≠‡∏á‡∏Ñ‡∏∏‡∏ì‡∏ó‡∏µ‡πà‡∏ô‡∏µ‡πà
                        frame_times.append(time.perf_counter())
                        
                        if len(frame_times) > 1:
                            # ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì FPS ‡∏à‡∏≤‡∏Å‡πÄ‡∏ü‡∏£‡∏°‡∏•‡πà‡∏≤‡∏™‡∏∏‡∏î
                            fps = len(frame_times) / (frame_times[-1] - frame_times[0])
                            print(f"FPS: {fps:.2f}")
                elif self.sensor.is_pressed and not self.is_triggered:
                    self.is_triggered = True
                    # QThread.msleep(100)
                    # sleep(0.2)
                    self.updateDetectImage.emit()
                elif not self.sensor.is_pressed:  # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏™‡∏ñ‡∏≤‡∏ô‡∏∞‡∏õ‡∏∏‡πà‡∏°‡πÅ‡∏ö‡∏ö polling
                    self.is_triggered = False

                QThread.msleep(10)
        except Exception as err:
            if hasattr(self, "picam2"):
                self.camera.close()  # ‡∏´‡∏≤‡∏Å‡πÉ‡∏ä‡πâ PiCamera ‡∏Ñ‡∏ß‡∏£‡∏õ‡∏¥‡∏î‡∏Å‡∏≤‡∏£‡πÄ‡∏ä‡∏∑‡πà‡∏≠‡∏°‡∏ï‡πà‡∏≠‡∏î‡πâ‡∏ß‡∏¢
            print("Camera resources released.")

    def close(self):
        self.thead_running = False
        self.camera.close()
